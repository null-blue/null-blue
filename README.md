I study how capability pathways, training data structures, and narrative framing create non-obvious risk surfaces in AI systems â€” with particular focus on role conditioning, corpus poisoning, and latent alignment drift. My goal is to help map the places where safety systems may quietly fail before those failures scale to global levels.
