Studying how capability pathways, training data structures, and narrative framing create non-obvious risk surfaces in AI systems, with particular focus on role conditioning, corpus poisoning, and latent alignment drift. My goal is to help map the places where safety systems may quietly fail before those failures scale to global levels.

> This project is dedicated to keeping adversarial literacy accessible to people like me, queer, trans, neurodivergent, or anyone whoâ€™s ever been pushed out of culture for not fitting the script.

